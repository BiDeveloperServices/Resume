<h1>Crystie Johnson <br/><a href="https://github.com/BiDeveloperServices"><a href="https://www.linkedin.com/in/MrsCrystie/"> Sr. Data Engineer / Data Architect Professional</a></h1>

<h2>üë®‚Äçüíª Software Development Projects:</h2>

- <b>Data Structures and Algorithms Practice (AlgoExpert)</b>
  - [Praciting DS & Algos in Python](https://github.com/BiDeveloperServices/Algorithms-Practice)
- <b>Full Stack Web App (React, NodeJS, Azure, and Machine Learning Components)</b>

<h2> ü§≥ Connect with me:</h2>
www.LinkedIn.com/in/MrsCrystie<br>
BIDeveloperServices@gmail.com<br> 
Atlanta, GA ‚ñ™ 1+ 470-377-1316
<br><br>

## Professional Summary <br>
For over twenty years, my career focus has been centered around Business Intelligence and Data Warehouse system design, with comprehensive hands-on 
expertise in managing large-scale database systems, Software Development, Data Warehouse, Data Migrations / Integrations, and ETL Architecture, Big Data, 
SQL / ETL Development, Report Development and Data Visualization, with the ability to apply technical skills for business resolutions and the ability to 
switch easily between different projects with the flexibility to handle rapidly changing environments.  Thus, allowing the application of extensive experience
across multiple Database types and platforms and provided a strong aptitude for multidimensional database design coupled with robust operational architectures
Strong leader and collaborator with a track record of delivering innovative data solutions that support strategic business goals. Creative thinker with the 
ability to think outside of the box to resolve complex issues.
<br><br>

## Technical Skills <br><br>
**Database Platforms	Languages** <br>
‚Ä¢	SQL Server 2019 and older versions	ÔÇß	T-SQL, PL/SQL<br>
‚Ä¢	MS Visual Studio 2019 and older versions 	ÔÇß	UNIX Shell Script, XML, Batch Files<br>
‚Ä¢	Oracle 8.05 / 9i / 10g / 11g / 12c<br>
‚Ä¢	Teradata v13, MYSQL v5.6	ÔÇß	Course work ‚Äì C++, Python Perl, and .Net<br>
‚Ä¢	TFS (Team Foundation Services),  Azure Data Catalog	ÔÇß	Knowledge - Java, JavaScript, C, C++, XHTML, HTML<br>
‚Ä¢	UNIX/Linux
<br><br>

**BI / ETL Tools**<br>
‚Ä¢	Microsoft ‚Äì SSMS, SSIS, SSAS<br>
‚Ä¢	Oracle ODI, Informatica v9.2, Wherescape Red, Pentaho (Kettle/Spoon)<br>
‚Ä¢	Azure Data Factory / DevOps (TFS - Microsoft Teams Foundation Server)<br>
‚Ä¢	Knowledge of Enterprise API Integration (SOAP/REST, WebSocket, REST, RPC)<br>
‚Ä¢	API Web Services (JSON, XML - Files)<br>
‚Ä¢	TOAD v12.1, Data Junction, DataStage and DJ Cosmos<br>
‚Ä¢	Conference tools ‚Äì Teams, Zoom and Skype<br>
‚Ä¢	Oracle PL/SQL Developer, SQL Navigator, SQL Plus<br>
‚Ä¢	Erwin r7, Power Designer, Visio, Siebel Analytics<br>
‚Ä¢	Version Control (SVN Tortoise), SourceSafe, GitHub, Bitbucket, DevOps (formerly TFS)<br>
‚Ä¢	Scheduler (Tivoli) and many other analytical tools
<br><br>

**Reporting and Visualization Tools**<br>
‚Ä¢	Microsoft ‚Äì SSRS, Tableau and PowerBI Desktop Services<br>
‚Ä¢	Pentaho report Designer (PROD and CDE), SharePoint<br>
‚Ä¢	Microsoft Office All versions (Excel, Access, and PowerPoint)<br>
‚Ä¢	SFTP - WinSCP, Putty Pigeon, FileZilla
<br><br>

## üë©üèº‚Äçüíª Professional Experience 
**Dassault Falcon Jet, Atlanta, GA** 
Sr. SQL Developer / Data Architect - Jan. 2023 ‚Äì April 2024<br> 
Sr. SQL Developer / Data Architect - Responsible for analyzing and articulating complex system requirements and the subsequent development of Databases, Schemas, Data Visualization, ETL Development, Reporting, analytical, and dashboard solutions for the Business intelligence (BI) platform spanning business domains across Dassault systems. Agile (Kanban /Scrum). This opportunity was100% remote.

‚Ä¢	Analyzed complex system requirements and developed databases, schemas, data visualization, ETL, reporting, and dashboard solutions for the BI platform. <br>
‚Ä¢	Led the design and implementation of data pipelines and ETL processes using Azure DevOps, SSIS, Wherescape Red, and complex stored procedures, optimizing data integration and transformation workflows.<br>
‚Ä¢	Created and designed ETL Framework for DataMart and the refactoring of data warehouse.  Developed data models and schemas to support business intelligence initiatives and reporting requirements.<br>
‚Ä¢	Designed and implemented scalable database solutions for a high-volume SQL Server platform, resulting in a 30% improvement in query performance.<br> 
‚Ä¢	Conducted regular performance tuning and query optimization to maintain database integrity and optimize application performance. 
‚Ä¢	Collaborated with cross-functional teams including software developers and system architects to integrate database solutions into larger projects.  Also, collaborated with business stakeholders to define data requirements and translate them into scalable data solutions.<br>
‚Ä¢	Analyzing SAP and Oracle legacy system to build requirements in order to convert to SQL Server.<br>
‚Ä¢	Completed reverse engineering of SAP, Wherescape Red and Qlik Sense systems to build SQL SERVER Datamart. Extracted table definitions, transformations, joins, keys to create mapping.<br>
‚Ä¢	Azure DevOps ‚Äì to build Stories and track Business requirement validation through Epic, Features and Task. Shared files and notes with Team via Boards.<br>
‚Ä¢	Azure - Created Branches to promote from DEV to Staging, QA and PROD via GIT, using Pull and Push request.<br>
‚Ä¢	Architected and deployed ETL solutions using Wherescape Red and SSIS.<br> 
‚Ä¢	Enhanced query performance by 30% through scalable database solutions.<br> 
‚Ä¢	Led a team of three junior developers, improving team efficiency by 20%.<br> 
‚Ä¢	Conducted performance tuning and query optimization.<br>
‚Ä¢	Worked remotely and collaborated with colleagues in France.
<br><br>

**Fortna (Formerly MHS Global), Atlanta, GA** 
Sr. SQL Developer / Data Architect - Oct. 2021 ‚Äì Jan. 2023<br>
Sr. SQL Developer / Data Architect - Responsible for analyzing and articulating complex system requirements and the subsequent development of Databases, Schemas, Data Visualization, ETL Development, Reporting, analytical, and dashboard solutions for the Business intelligence (BI) platform spanning business domains across MHS. Agile (Kanban /Scrum). Successfully migrated 30+ applications to Azure. Implemented DevOps best practices that reduced deployment times by 80% and increased application availability to 99.9%. This opportunity was100% remote. 

‚Ä¢	Migrated 30+ applications to Azure, reducing deployment times by 80%.<br> 
‚Ä¢	Devised high-quality database solutions ranging in size and complexity, increasing productivity, and improving data sharing. Used Azure Synapse Analytics for real-time Big data analytics, reducing the time to insight by 30%.<br> 
‚Ä¢	Worked on migration of data from On -Prem SQL Server to cloud databases (Azure synapsis analytics (DW) and Azure SQL DB).<br>
‚Ä¢	Integrating various Azure services into the application architecture, such as Azure Functions, Azure App Service, Azure SQL Database, Azure Storage, Azure Logic Apps, and more.<br>
‚Ä¢	Collaborated with the stakeholders and team to support existing functionality and build, implement, and support new solutions using SQL server, T-SQL programming, XML, C++, Infor BODS.<br>
‚Ä¢	Developed custom database objects, stored procedures, and delivered application support by writing Python code via VB and PyCharm, batch files and C++.<br>
‚Ä¢	Contributed to project management activities, such as scoping, estimating, and planning, to ensure critical milestones are met with high quality deliverables.<br> 
‚Ä¢	Responsible for Database design, development, testing, validation, performance tuning and quality assurance activities for multiple business intelligence projects.<br>
‚Ä¢	Built complex data transformations with SSIS including ingestion of data files from various sources (EBCDIC, ASCII, XML, JSON files, unstructured datasets, and other data types), moving data from legacy database (Source) platform to Target (SQL Server or Oracle).<br>
‚Ä¢	Designed and optimized data pipelines for large-scale data processing tasks, ensuring efficient data flow and processing.<br>
‚Ä¢	Devised high-quality database solutions using Azure Synapse Analytics.  Integrating various Azure services into application architecture.<br> 
‚Ä¢	Developed custom database objects and provided application support.<br> 
ÔÇß	Maintained SQL DW databases and ETL processes. Resolved complex business issues, proposing long-term solutions to avoid repeat problems.<br>
‚Ä¢	Utilized Azure DevOps for project tracking and code management. 
<br><br>

**NSSP - CDC, Atlanta, GA** 
Sr. SQL Developer / Data Architect - Mar. 2020 ‚Äì Oct. 2021<br> 
Sr. SQL Developer / Data Architect - Responsible for designing, implementing, and supporting mission-critical public health syndromic surveillance reporting solutions in a cloud-based environment. Development of ETL Framework, data warehouse, business intelligence, analytic, and reporting applications to ensure the availability, reliability, and scalability of the solution.  Agile (Kanban /Scrum).<br><br>
‚Ä¢	Designed and supported public health syndromic surveillance reporting solutions.<br> 
‚Ä¢	Developed data solutions using AWS data services on the EC2 platform.<br> 
‚Ä¢	Created/maintained databases and objects such as tables, constraints, indexes, schemas, views, stored procedures, functions, triggers, etc. Utilized SQL Server SSMS, SSIS, Oracle, Informatica, SQL Developer and many other tools.<br>
‚Ä¢	Developed ETL Architecture Roadmaps and defined requirement infrastructure/projects to implement the roadmaps.<br>
‚Ä¢	Utilization of advanced skills in creating ETL Framework, Data Visualization, Database Development (Designed, Implementation, Monitoring, Modification and Maintenance), Reports, Anomaly Resolution and Requirement Analysis.<br>
‚Ä¢ Advanced experience extracting various types of data (EBCDIC, ASCII, XML, HL7, PII, JSON files, unstructured datasets, and other data types) from numerous, heterogeneous OLTP/OLAP and/or legacy data sources using programming code, batch files (.bat), BCP, SSIS, Informatica, Pentaho, DataStage and other applications‚Ä¶then transform into the targeted structures such as DB Tables, Reports and/or MS Excel.<br> 
‚Ä¢	Modified existing table structures to resolve anomalies and enhance performance by applying indexes (Columnstore Clustered and Non-Clustered), creating schemas, views, stored procedures, functions, and triggers.<br>
‚Ä¢	Enhanced database performance through advanced indexing and query optimization.<br> 
‚Ä¢	Designed and developed RESTful APIs using Node.js and Express.<br> 
‚Ä¢	Collaborated with stakeholders, Developers, Angular Developers and other team members to build front and backend applications and implement API security measures.<br>
‚Ä¢	Designed and developed end-to-end ETL solutions for complex projects using Microsoft SQL Server Integration Services (SSIS) via Visual Studios v19 to obtain data from source systems then transform, cleanse, or enhance data; to provide data to OLAP systems data warehouse applications.<br>
‚Ä¢	Used Jira, SourceSafe, GIT and SharePoint to log code.<br>
‚Ä¢	Wrote complex T-SQL, PL/SQL, BTEQ programming, including SQL queries (DDL, DML, DCL, DAX, MDX and TCL), Packages, Procedures, Functions, Cursors, Triggers, Indexes, Partitions, workflows, etc... Worked on multiple platforms such as WINDOWS/UNIX/SOLARIS/LINUX -Users, User Groups, Folders, User Roles, Profiles & Privileges. Expertise in UNIX shell scripting using named functions, scripts, and SQL loader. Experience with secure file transfer methods, FileZilla, GIT, SFTP and WinSCP.
<br><br>

**JP Business Solutions, Inc.** 
Consultant - Nov. 1995 ‚Äì Mar. 2020<br> 
OPM (Government), CGI, AT&T, Home Depot, Auto Trader, SI-Ops, Cox Enterprise, Comcast, Coca-Coca-Cola Enterprise, Northrop Grumman, SPH Analytics, MiMedix, UPS, NBIS, CareDental, BBVA Bank, BellSouth Cellular Corp and BellSouth Mobility, PRG-Schultz, Bank of America, Nations Bank, Amoco Foam, Children's Hospital, Kaiser Permanente, Global Network Services.

‚Ä¢	Worked with various clients to create and maintain data solutions. <br>
‚Ä¢	Managed several data migrations / Integrations from legacy systems to the current systems which included Oracle, SQL Server, SAP, Teradata, MySQL, and various other systems.<br>
‚Ä¢	Expertise in designing and developing interfaces, packages, with Design, Development and implementation of the Data warehousing Projects using ETL Tools - Oracle data integrator (ODI), Informatica, SSIS, Cosmos and DBT tools.<br>
‚Ä¢	Experience in Development and support of Data warehouse applications Extraction, transformation, and loading (ETL) using ODI, Informatica, SSIS, Cosmos and DBT tools.<br> 
‚Ä¢	Utilized ODI designer for designing the interfaces, defining the data stores, interfaces and. packages, used variable and modified the ODI knowledge modules (RKM, LKM, I KM, CKM, JKM) to create interfaces for data cleansing, load, and transformation of data from the source to the target databases. Configuration of TNS names and parameter files. ‚ñ™ Collaborated closely with clients to analyze IT system requirements, client information technology needs and their resources in order to plan IT projects and client fulfillment expectations such as Life Cycle development of Datawarehouse and Datamarts.<br>
‚Ä¢	Developed comprehensive data models and database schemas.<br> 
‚Ä¢	Designed and implemented robust ETL processes using tools like Spark and Airflow, optimizing data workflows and improving efficiency.<br>
‚Ä¢	Extensive work in ETL Framework (SSIS, SSAS, ODI, Informatica and Coding) development including data mappings and roadmap, workflow development, building cubes, process monitoring, performance tuning, testing, job scheduling and implementation/deployment of ETL processes using various tools and programming code. Design, develop, and maintain efficient and scalable data pipelines to collect, process, and store data from various sources.<br>
‚Ä¢	Advance T-SQL and PL/SQL, BTEQ, DAX, MDX programming, including Packages, Procedures, Functions, Cursors & Triggers. Worked on multiple platforms such as Windows/Unix/Solaris/Linux -Users, User Groups, Folders, User Roles, Profiles & Privileges. Expertise in UNIX shell scripting using named functions, scripts, and SQL loader. Experience with secure file transfer methods, FileZilla, GIT, SFTP, WinSCP, MFT.<br>
‚Ä¢	Utilized advanced skills in ETL framework, data visualization, and database development.  Conducted root cause analysis and resolved data anomalies.<br> 
‚Ä¢	Led the administration of Azure Databricks and other Azure services, ensuring security and compliance in cloud infrastructure.<br>
‚Ä¢	Worked extensively with the Spark ecosystem, including Spark SQL, DataFrames, and Datasets, to develop scalable data processing solutions.<br>
‚Ä¢	Managed Streaming ETL processes using Kafka, Flink, and Spark Streaming, enabling real-time data processing for critical applications.<br>
‚Ä¢	Coordinated with stakeholders to gather requirements, define data architecture, and deliver data solutions that met business needs.
<br><br>

## üë©üèº‚Äçüéì Education
Bachelor of Science in Computer Science<br>
University of Georgia
1991 
<br><br>

**Additional Skills**<br>
‚Ä¢	Database Development: Star/Snowflake schema, views, stored procedures, constraints, triggers, joins, cursors, CTE, temp tables, table variables, functions, batch scripts, workflows.<br>
‚Ä¢	Writing complex T-SQL/PL-SQL code, ETL processes, file transfer, data discovery, data profiling, programming, data modeling, database design, root cause analysis, reporting and analytics, SDLC, performance tuning, and optimization.<br>
‚Ä¢	Performed a wide range of technical and analytical work related to DW/DM development, maintenance, and processes. Provided leadership, subject matter expertise and project management to multiple projects at various stages in the requirements process and various levels of complexity while consistently meeting deadlines, exceeding expectations, and producing high-quality deliverables on time and on budget. Proficient in SQL Server, Oracle, Teradata, Azure Data Factory and MySQL databases (Development/Admin Level).
<br><br>

